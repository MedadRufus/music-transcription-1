{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 980M (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Activation, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Input, Model, load_model, model_from_json\n",
    "from librosa import cqt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from warnings import warn\n",
    "from zipfile import ZipFile\n",
    "\n",
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from music_transcription.pitch_detection.cnn_cqt_pitch_detection import CnnCqtFeatureExtractor\n",
    "from music_transcription.pitch_detection.read_data import get_wav_and_truth_files, read_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS_CV = {1, 2}\n",
    "DATASETS_ADDITIONAL = {3, 9, 10, 11}\n",
    "\n",
    "sample_rate = 44100\n",
    "subsampling_step = 1\n",
    "min_pitch = 40\n",
    "max_pitch = 88\n",
    "onset_group_threshold_seconds = 0.05\n",
    "\n",
    "image_data_format = 'channels_first'\n",
    "cqt_configs = [\n",
    "    {\n",
    "        'hop_length': 512,\n",
    "        'fmin': 55.0,\n",
    "        'n_bins': 180,\n",
    "        'bins_per_octave': 36,\n",
    "        'scale': False,\n",
    "    },\n",
    "]\n",
    "n_frames_before = 15\n",
    "n_frames_after = 20\n",
    "\n",
    "LOSS = 'binary_crossentropy'\n",
    "OPTIMIZER = 'adam'\n",
    "METRICS = None\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping AR_Lick11_FN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping AR_Lick11_KN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping AR_Lick11_MN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:135: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\audio\\desktop.ini, not a .wav file.\n",
      "  warn('Skipping ' + path_to_wav + ', not a .wav file.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping FS_Lick11_FN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping FS_Lick11_KN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping FS_Lick11_MN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping LP_Lick11_FN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping LP_Lick11_KN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping LP_Lick11_MN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G63-44104-1111-20675.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G71-40100-1111-20749.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G83-45105-1111-20988.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G91-43103-1111-21064.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G93-46106-1111-21145.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping P94-43120-1111-41410.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping P94-44110-1111-41396.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n"
     ]
    }
   ],
   "source": [
    "wav_file_paths_cv, truth_dataset_format_tuples_cv = get_wav_and_truth_files(DATASETS_CV)\n",
    "wav_file_paths_additional, truth_dataset_format_tuples_additional = get_wav_and_truth_files(DATASETS_ADDITIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\AR_NH_IV.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\AR_NH_IX.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\FS_NH_IV.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\FS_NH_IX.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\LP_NH_IV.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:56: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset3\\audio\\pathetique_poly.wav, cannot handle stereo signal.\n",
      "  warn('Skipping ' + path_to_wav + ', cannot handle stereo signal.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset3\\annotation\\pathetique_poly.xml, pitch 30 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(515965, 180)\n",
      "3.63677319949\n",
      "22.0932928692\n",
      "Standardizing for each spectrogram and bin\n",
      "-2.02342757837e-16\n",
      "1.0\n",
      "(4466, 36, 180)\n",
      "Reshaping data\n",
      "(4466, 1, 36, 180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\LP_NH_IX.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spectrograms\n",
      "(73188, 180)\n",
      "5.06737530463\n",
      "27.5702815749\n",
      "Standardizing for each spectrogram and bin\n",
      "0.0824521316465\n",
      "1.19121556558\n",
      "(633, 36, 180)\n",
      "Reshaping data\n",
      "(633, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(527000, 180)\n",
      "3.56672113525\n",
      "21.6869298092\n",
      "Standardizing for each spectrogram and bin\n",
      "-2.03703898456e-16\n",
      "1.0\n",
      "(4404, 36, 180)\n",
      "Reshaping data\n",
      "(4404, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "(62153, 180)\n",
      "5.91534937004\n",
      "30.9263472148\n",
      "Standardizing for each spectrogram and bin\n",
      "0.149010008662\n",
      "1.42025840175\n",
      "(695, 36, 180)\n",
      "Reshaping data\n",
      "(695, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(504083, 180)\n",
      "3.68388396785\n",
      "22.797992656\n",
      "Standardizing for each spectrogram and bin\n",
      "1.54527309082e-16\n",
      "1.0\n",
      "(4167, 36, 180)\n",
      "Reshaping data\n",
      "(4167, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "(85070, 180)\n",
      "4.58840326211\n",
      "23.1412538061\n",
      "Standardizing for each spectrogram and bin\n",
      "0.0728228072821\n",
      "1.1014277565\n",
      "(932, 36, 180)\n",
      "Reshaping data\n",
      "(932, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(510210, 180)\n",
      "3.43382394529\n",
      "21.3277811016\n",
      "Standardizing for each spectrogram and bin\n",
      "1.65859997357e-16\n",
      "1.0\n",
      "(4236, 36, 180)\n",
      "Reshaping data\n",
      "(4236, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "(78943, 180)\n",
      "6.274748015\n",
      "30.8186584223\n",
      "Standardizing for each spectrogram and bin\n",
      "0.18061681751\n",
      "1.4366928998\n",
      "(863, 36, 180)\n",
      "Reshaping data\n",
      "(863, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(514789, 180)\n",
      "3.66276036624\n",
      "22.5576229671\n",
      "Standardizing for each spectrogram and bin\n",
      "-2.75806647857e-16\n",
      "1.0\n",
      "(4387, 36, 180)\n",
      "Reshaping data\n",
      "(4387, 1, 36, 180)\n",
      "Creating spectrograms\n",
      "(74364, 180)\n",
      "4.8648539817\n",
      "24.7547277884\n",
      "Standardizing for each spectrogram and bin\n",
      "0.0572328367426\n",
      "1.01383766925\n",
      "(712, 36, 180)\n",
      "Reshaping data\n",
      "(712, 1, 36, 180)\n"
     ]
    }
   ],
   "source": [
    "folds = []\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for k, (train_indices, test_indices) in enumerate(k_fold.split(wav_file_paths_cv)):\n",
    "    # if k > 0:\n",
    "    #     print('Skipping split {}'.format(k))\n",
    "    #     continue\n",
    "    \n",
    "    wav_file_paths_train = [wav_file_paths_cv[i] for i in train_indices] + wav_file_paths_additional\n",
    "    truth_dataset_format_tuples_train = [truth_dataset_format_tuples_cv[i] for i in train_indices] + truth_dataset_format_tuples_additional\n",
    "    wav_file_paths_test = [wav_file_paths_cv[i] for i in test_indices]\n",
    "    truth_dataset_format_tuples_test = [truth_dataset_format_tuples_cv[i] for i in test_indices]\n",
    "    \n",
    "    data_train, y_train, wav_file_paths_train_valid, truth_dataset_format_tuples_train_valid = read_data_y(\n",
    "        wav_file_paths_train, truth_dataset_format_tuples_train,\n",
    "        sample_rate, subsampling_step,\n",
    "        min_pitch, max_pitch,\n",
    "        onset_group_threshold_seconds=onset_group_threshold_seconds\n",
    "    )\n",
    "    \n",
    "    feature_extractor = CnnCqtFeatureExtractor(image_data_format, sample_rate, cqt_configs, n_frames_before, n_frames_after)\n",
    "    list_of_X_train, sample_file_indexes_train = feature_extractor.fit_transform(data_train)\n",
    "\n",
    "    data_test, y_test, wav_file_paths_test_valid, truth_dataset_format_tuples_test_valid = read_data_y(\n",
    "        wav_file_paths_test, truth_dataset_format_tuples_test,\n",
    "        sample_rate, subsampling_step,\n",
    "        min_pitch, max_pitch,\n",
    "        onset_group_threshold_seconds=onset_group_threshold_seconds\n",
    "    )\n",
    "    list_of_X_test, sample_file_indexes_test = feature_extractor.transform(data_test, verbose=True)\n",
    "\n",
    "    # if self.config['sample_weights'] == 'balanced':\n",
    "        # validation_data = (list_of_X_test, y_test, self._get_sample_weights(sample_file_indexes_test,\n",
    "        #                                                                     truth_dataset_format_tuples_test_valid))\n",
    "    # else:\n",
    "    \n",
    "    folds.append((list_of_X_train, y_train, list_of_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = defaultdict(int)\n",
    "for ds in [t[1] for t in truth_dataset_format_tuples_cv]:\n",
    "    counts[ds] += 1\n",
    "print(counts)\n",
    "\n",
    "for k, (train_indices, test_indices) in enumerate(k_fold.split(wav_file_paths_cv)):\n",
    "    print(k)\n",
    "    counts_test_k = defaultdict(int)\n",
    "    for ds in [t[1] for t in [truth_dataset_format_tuples_cv[i] for i in test_indices]]:\n",
    "        counts_test_k[ds] += 1\n",
    "    print(counts_test_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, proba_threshold, list_of_X, y, epsilon=1e-7):\n",
    "    proba_matrix = model.predict(list_of_X)\n",
    "    y = proba_matrix > proba_threshold\n",
    "    y = y.astype(np.int8)\n",
    "\n",
    "    # Make sure at least one pitch is returned.\n",
    "    for probas, labels in zip(proba_matrix, y):\n",
    "        if labels.sum() == 0:\n",
    "            max_proba = max(probas)\n",
    "            max_index = np.where(np.logical_and(probas > max_proba - epsilon, probas < max_proba + epsilon))[0][0]\n",
    "            labels[max_index] = 1\n",
    "\n",
    "    return y\n",
    "\n",
    "def print_metrics(y, y_predicted):\n",
    "    print('Accuracy: {}'.format(accuracy_score(y, y_predicted)))\n",
    "    print(classification_report(y, y_predicted,\n",
    "                                target_names=[str(pitch) for pitch in range(min_pitch, max_pitch + 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_1(list_of_X, n_output_units):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(20, (7, 3), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Conv2D(20, (3, 3), padding='valid')(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(0.25)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(0.5)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_2(list_of_X, n_output_units, dropout_conv=0.25, dropout_dense=0.5):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(10, (7, 3), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Conv2D(20, (3, 3), padding='valid')(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(dropout_conv)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(dropout_dense)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_3(list_of_X, n_output_units):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(49, (16, 6), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 29))(conv)\n",
    "        conv = Dropout(0.25)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(32)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(0.5)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_4(list_of_X, n_output_units, dropout_conv=0.25, dropout_dense=0.5):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(10, (7, 3), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(dropout_conv)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(dropout_dense)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    # model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_5(list_of_X, n_output_units,\n",
    "                   dropout_conv, dropout_dense,\n",
    "                   n_filters, filter_size, pool_size):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(n_filters, filter_size, padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=pool_size)(conv)\n",
    "        conv = Dropout(dropout_conv)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(dropout_dense)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_6(list_of_X, n_output_units):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(10, (15, 6), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_7(list_of_X, n_output_units, dropout_conv=0.1, dropout_dense=0.2):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(10, (15, 6), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = Conv2D(20, (5, 2), padding='valid')(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(dropout_conv)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(dropout_dense)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    # model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_5\n",
      "dropout_conv=0.25, dropout_dense=0.5, proba_threshold=0.5, n_filters=50, filter_size=(6, 3), pool_size=(6, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_116 (InputLayer)       (None, 1, 36, 180)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 50, 31, 178)       950       \n",
      "_________________________________________________________________\n",
      "activation_240 (Activation)  (None, 50, 31, 178)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_121 (MaxPoolin (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "dropout_223 (Dropout)        (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "flatten_116 (Flatten)        (None, 14750)             0         \n",
      "_________________________________________________________________\n",
      "dense_231 (Dense)            (None, 256)               3776256   \n",
      "_________________________________________________________________\n",
      "activation_241 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_224 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_232 (Dense)            (None, 49)                12593     \n",
      "=================================================================\n",
      "Total params: 3,789,799.0\n",
      "Trainable params: 3,789,799.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 4466 samples, validate on 633 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.2486 - val_loss: 0.0667\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.1028 - val_loss: 0.0423\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.0702 - val_loss: 0.0269\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.0525 - val_loss: 0.0221\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.0441 - val_loss: 0.0212\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0377 - val_loss: 0.0180\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0328 - val_loss: 0.0164\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.0308 - val_loss: 0.0157\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0270 - val_loss: 0.0148\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.0246 - val_loss: 0.0137\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0228 - val_loss: 0.0134\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0218 - val_loss: 0.0112\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0203 - val_loss: 0.0114\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0195 - val_loss: 0.0115\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0183 - val_loss: 0.0113\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0178 - val_loss: 0.0105\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0164 - val_loss: 0.0100\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0157 - val_loss: 0.0102\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0154 - val_loss: 0.0102\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0151 - val_loss: 0.0109\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0144 - val_loss: 0.0108\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0139 - val_loss: 0.0094\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0128 - val_loss: 0.0092\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0126 - val_loss: 0.0101\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0123 - val_loss: 0.0095\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0120 - val_loss: 0.0095\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0112 - val_loss: 0.0088\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0114 - val_loss: 0.0083\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0112 - val_loss: 0.0090\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0107 - val_loss: 0.0091\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0101 - val_loss: 0.0092\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0099 - val_loss: 0.0094\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0096 - val_loss: 0.0091\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0092 - val_loss: 0.0082\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0087 - val_loss: 0.0080\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0090 - val_loss: 0.0081\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0082 - val_loss: 0.0094\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0083 - val_loss: 0.0096\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0082 - val_loss: 0.0090\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0082 - val_loss: 0.0091\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0085 - val_loss: 0.0086\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0080 - val_loss: 0.0100\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0074 - val_loss: 0.0090\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0075 - val_loss: 0.0083\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0118\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0102\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0074 - val_loss: 0.0092\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0076 - val_loss: 0.0087\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0065 - val_loss: 0.0095\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0062 - val_loss: 0.0089\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0060 - val_loss: 0.0104\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0059 - val_loss: 0.0098\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0106\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0063 - val_loss: 0.0104\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0056 - val_loss: 0.0096\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0056 - val_loss: 0.0101\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0058 - val_loss: 0.0105\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0062 - val_loss: 0.0096\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0061 - val_loss: 0.0103\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0057 - val_loss: 0.0095\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0058 - val_loss: 0.0094\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0057 - val_loss: 0.0092\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0057 - val_loss: 0.0098\n",
      "Accuracy: 0.9415481832543444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        13\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         5\n",
      "         44       1.00      1.00      1.00         4\n",
      "         45       1.00      1.00      1.00        29\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      1.00      1.00        21\n",
      "         48       1.00      1.00      1.00        59\n",
      "         49       1.00      0.93      0.97        15\n",
      "         50       1.00      1.00      1.00        43\n",
      "         51       1.00      1.00      1.00         6\n",
      "         52       1.00      1.00      1.00        52\n",
      "         53       0.95      0.95      0.95        19\n",
      "         54       1.00      0.97      0.98        32\n",
      "         55       0.98      0.96      0.97        45\n",
      "         56       1.00      0.95      0.97        38\n",
      "         57       0.98      0.96      0.97        45\n",
      "         58       1.00      0.80      0.89         5\n",
      "         59       1.00      0.96      0.98        73\n",
      "         60       1.00      0.85      0.92        39\n",
      "         61       0.98      0.91      0.94        46\n",
      "         62       0.95      0.93      0.94        45\n",
      "         63       1.00      0.75      0.86         8\n",
      "         64       1.00      0.97      0.98        63\n",
      "         65       1.00      0.87      0.93        15\n",
      "         66       1.00      1.00      1.00        14\n",
      "         67       0.92      0.67      0.77        18\n",
      "         68       1.00      0.62      0.77         8\n",
      "         69       1.00      0.90      0.95        39\n",
      "         70       0.83      0.83      0.83         6\n",
      "         71       0.75      0.60      0.67         5\n",
      "         72       1.00      0.80      0.89        10\n",
      "         73       1.00      0.50      0.67         4\n",
      "         74       1.00      0.83      0.91        12\n",
      "         75       1.00      1.00      1.00         3\n",
      "         76       1.00      1.00      1.00         4\n",
      "         77       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         2\n",
      "         79       1.00      1.00      1.00         3\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       1.00      1.00      1.00         3\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      1.00      1.00         2\n",
      "\n",
      "avg / total       0.99      0.94      0.96       862\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_117 (InputLayer)       (None, 1, 36, 180)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 50, 31, 178)       950       \n",
      "_________________________________________________________________\n",
      "activation_242 (Activation)  (None, 50, 31, 178)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_122 (MaxPoolin (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "dropout_225 (Dropout)        (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "flatten_117 (Flatten)        (None, 14750)             0         \n",
      "_________________________________________________________________\n",
      "dense_233 (Dense)            (None, 256)               3776256   \n",
      "_________________________________________________________________\n",
      "activation_243 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_226 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_234 (Dense)            (None, 49)                12593     \n",
      "=================================================================\n",
      "Total params: 3,789,799.0\n",
      "Trainable params: 3,789,799.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 4404 samples, validate on 695 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.2647 - val_loss: 0.0762\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.1106 - val_loss: 0.0508\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.0766 - val_loss: 0.0341\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.0593 - val_loss: 0.0274\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.0489 - val_loss: 0.0215\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0424 - val_loss: 0.0190\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0377 - val_loss: 0.0170\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.0341 - val_loss: 0.0156\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0319 - val_loss: 0.0155\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.0289 - val_loss: 0.0138\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0267 - val_loss: 0.0130\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0252 - val_loss: 0.0127\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0237 - val_loss: 0.0116\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0222 - val_loss: 0.0112\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0204 - val_loss: 0.0114\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0195 - val_loss: 0.0116\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0187 - val_loss: 0.0112\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0182 - val_loss: 0.0105\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0174 - val_loss: 0.0106\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0167 - val_loss: 0.0113\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0154 - val_loss: 0.0102\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0156 - val_loss: 0.0102\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0150 - val_loss: 0.0097\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0141 - val_loss: 0.0105\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0135 - val_loss: 0.0101\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0130 - val_loss: 0.0097\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0139 - val_loss: 0.0100\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0126 - val_loss: 0.0099\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0127 - val_loss: 0.0101\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0123 - val_loss: 0.0105\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0120 - val_loss: 0.0101\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0122 - val_loss: 0.0106\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0117 - val_loss: 0.0095\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0110 - val_loss: 0.0099\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0103 - val_loss: 0.0109\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0100 - val_loss: 0.0102\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0095 - val_loss: 0.0091\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0091 - val_loss: 0.0111\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0092 - val_loss: 0.0111\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0089 - val_loss: 0.0109\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0081 - val_loss: 0.0108\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0080 - val_loss: 0.0106\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0084 - val_loss: 0.0107\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0076 - val_loss: 0.0099\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0079 - val_loss: 0.0097\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0083 - val_loss: 0.0104\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0079 - val_loss: 0.0116\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0079 - val_loss: 0.0094\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0078 - val_loss: 0.0103\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0077 - val_loss: 0.0106\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0074 - val_loss: 0.0105\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0082 - val_loss: 0.0114\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0071 - val_loss: 0.0111\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0070 - val_loss: 0.0113\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0111\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0100\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0071 - val_loss: 0.0107\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0067 - val_loss: 0.0104\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0070 - val_loss: 0.0107\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0120\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0070 - val_loss: 0.0121\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0066 - val_loss: 0.0108\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0065 - val_loss: 0.0110\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0117\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0067 - val_loss: 0.0111\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0059 - val_loss: 0.0103\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0071 - val_loss: 0.0106\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0108\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0062 - val_loss: 0.0113\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0062 - val_loss: 0.0107\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0066 - val_loss: 0.0111\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0056 - val_loss: 0.0105\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0055 - val_loss: 0.0106\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0058 - val_loss: 0.0104\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0060 - val_loss: 0.0105\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0056 - val_loss: 0.0101\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0061 - val_loss: 0.0108\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0054 - val_loss: 0.0101\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0060 - val_loss: 0.0102\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0057 - val_loss: 0.0104\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0057 - val_loss: 0.0106\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0055 - val_loss: 0.0095\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0052 - val_loss: 0.0111\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0053 - val_loss: 0.0126\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0057 - val_loss: 0.0123\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0051 - val_loss: 0.0104\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0052 - val_loss: 0.0114\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0057 - val_loss: 0.0109\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0052 - val_loss: 0.0121\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0056 - val_loss: 0.0121\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.0051 - val_loss: 0.0122\n",
      "Epoch 100/1000\n",
      "1s - loss: 0.0053 - val_loss: 0.0113\n",
      "Epoch 101/1000\n",
      "1s - loss: 0.0053 - val_loss: 0.0109\n",
      "Accuracy: 0.9366906474820144\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        14\n",
      "         41       1.00      1.00      1.00         5\n",
      "         42       1.00      1.00      1.00         1\n",
      "         43       1.00      1.00      1.00        11\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      1.00      1.00        33\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      1.00      1.00        19\n",
      "         48       1.00      0.98      0.99        58\n",
      "         49       1.00      1.00      1.00        13\n",
      "         50       1.00      1.00      1.00        59\n",
      "         51       1.00      1.00      1.00         8\n",
      "         52       1.00      0.97      0.98        62\n",
      "         53       1.00      0.94      0.97        16\n",
      "         54       0.98      1.00      0.99        42\n",
      "         55       0.91      0.95      0.93        42\n",
      "         56       1.00      0.97      0.99        36\n",
      "         57       0.98      0.89      0.93        56\n",
      "         58       0.88      0.70      0.78        10\n",
      "         59       0.99      0.96      0.97        71\n",
      "         60       0.93      0.93      0.93        41\n",
      "         61       0.98      0.96      0.97        49\n",
      "         62       0.98      0.91      0.94        64\n",
      "         63       0.56      0.83      0.67         6\n",
      "         64       1.00      0.95      0.97        57\n",
      "         65       1.00      0.83      0.91        18\n",
      "         66       0.95      0.95      0.95        20\n",
      "         67       0.89      0.86      0.88        29\n",
      "         68       1.00      0.83      0.91         6\n",
      "         69       1.00      0.86      0.92        43\n",
      "         70       0.67      0.40      0.50         5\n",
      "         71       1.00      0.80      0.89         5\n",
      "         72       0.90      1.00      0.95         9\n",
      "         73       1.00      0.80      0.89         5\n",
      "         74       1.00      0.93      0.96        14\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       1.00      1.00      1.00         2\n",
      "         77       0.00      0.00      0.00         0\n",
      "         78       0.00      0.00      0.00         0\n",
      "         79       0.00      0.00      0.00         0\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       0.00      0.00      0.00         0\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.98      0.94      0.96       936\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_118 (InputLayer)       (None, 1, 36, 180)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 50, 31, 178)       950       \n",
      "_________________________________________________________________\n",
      "activation_244 (Activation)  (None, 50, 31, 178)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_123 (MaxPoolin (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "dropout_227 (Dropout)        (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "flatten_118 (Flatten)        (None, 14750)             0         \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 256)               3776256   \n",
      "_________________________________________________________________\n",
      "activation_245 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_228 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 49)                12593     \n",
      "=================================================================\n",
      "Total params: 3,789,799.0\n",
      "Trainable params: 3,789,799.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 4167 samples, validate on 932 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.2608 - val_loss: 0.0786\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.1178 - val_loss: 0.0594\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.0842 - val_loss: 0.0465\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.0640 - val_loss: 0.0395\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.0521 - val_loss: 0.0362\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0445 - val_loss: 0.0334\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0395 - val_loss: 0.0324\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.0355 - val_loss: 0.0306\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0328 - val_loss: 0.0313\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.0301 - val_loss: 0.0314\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0287 - val_loss: 0.0317\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0265 - val_loss: 0.0292\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0246 - val_loss: 0.0307\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0237 - val_loss: 0.0300\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0234 - val_loss: 0.0282\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0220 - val_loss: 0.0291\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0210 - val_loss: 0.0297\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0199 - val_loss: 0.0311\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0189 - val_loss: 0.0293\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0186 - val_loss: 0.0309\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0178 - val_loss: 0.0288\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0168 - val_loss: 0.0302\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0174 - val_loss: 0.0305\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0165 - val_loss: 0.0281\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0159 - val_loss: 0.0312\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0146 - val_loss: 0.0330\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0145 - val_loss: 0.0299\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0147 - val_loss: 0.0301\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0136 - val_loss: 0.0296\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0137 - val_loss: 0.0318\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0133 - val_loss: 0.0319\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0131 - val_loss: 0.0324\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0134 - val_loss: 0.0323\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0123 - val_loss: 0.0322\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0119 - val_loss: 0.0321\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0120 - val_loss: 0.0337\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.0121 - val_loss: 0.0335\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.0115 - val_loss: 0.0324\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.0115 - val_loss: 0.0356\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.0105 - val_loss: 0.0331\n",
      "Epoch 41/1000\n",
      "1s - loss: 0.0110 - val_loss: 0.0326\n",
      "Epoch 42/1000\n",
      "1s - loss: 0.0106 - val_loss: 0.0313\n",
      "Epoch 43/1000\n",
      "1s - loss: 0.0106 - val_loss: 0.0320\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.0106 - val_loss: 0.0347\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.0105 - val_loss: 0.0360\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.0102 - val_loss: 0.0356\n",
      "Epoch 47/1000\n",
      "1s - loss: 0.0101 - val_loss: 0.0334\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.0097 - val_loss: 0.0354\n",
      "Epoch 49/1000\n",
      "1s - loss: 0.0094 - val_loss: 0.0352\n",
      "Epoch 50/1000\n",
      "1s - loss: 0.0090 - val_loss: 0.0354\n",
      "Epoch 51/1000\n",
      "1s - loss: 0.0091 - val_loss: 0.0347\n",
      "Epoch 52/1000\n",
      "1s - loss: 0.0092 - val_loss: 0.0373\n",
      "Epoch 53/1000\n",
      "1s - loss: 0.0090 - val_loss: 0.0375\n",
      "Epoch 54/1000\n",
      "1s - loss: 0.0090 - val_loss: 0.0332\n",
      "Epoch 55/1000\n",
      "1s - loss: 0.0088 - val_loss: 0.0378\n",
      "Epoch 56/1000\n",
      "1s - loss: 0.0090 - val_loss: 0.0362\n",
      "Epoch 57/1000\n",
      "1s - loss: 0.0088 - val_loss: 0.0344\n",
      "Epoch 58/1000\n",
      "1s - loss: 0.0091 - val_loss: 0.0338\n",
      "Epoch 59/1000\n",
      "1s - loss: 0.0087 - val_loss: 0.0357\n",
      "Epoch 60/1000\n",
      "1s - loss: 0.0083 - val_loss: 0.0403\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.0084 - val_loss: 0.0383\n",
      "Epoch 62/1000\n",
      "1s - loss: 0.0079 - val_loss: 0.0327\n",
      "Epoch 63/1000\n",
      "1s - loss: 0.0079 - val_loss: 0.0393\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.0078 - val_loss: 0.0390\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.0079 - val_loss: 0.0414\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.0078 - val_loss: 0.0389\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.0075 - val_loss: 0.0411\n",
      "Epoch 68/1000\n",
      "1s - loss: 0.0072 - val_loss: 0.0410\n",
      "Epoch 69/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0397\n",
      "Epoch 70/1000\n",
      "1s - loss: 0.0080 - val_loss: 0.0402\n",
      "Epoch 71/1000\n",
      "1s - loss: 0.0075 - val_loss: 0.0393\n",
      "Epoch 72/1000\n",
      "1s - loss: 0.0067 - val_loss: 0.0412\n",
      "Epoch 73/1000\n",
      "1s - loss: 0.0071 - val_loss: 0.0420\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.0068 - val_loss: 0.0420\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.0073 - val_loss: 0.0412\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.0065 - val_loss: 0.0430\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.0069 - val_loss: 0.0397\n",
      "Epoch 78/1000\n",
      "1s - loss: 0.0069 - val_loss: 0.0421\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.0071 - val_loss: 0.0402\n",
      "Epoch 80/1000\n",
      "1s - loss: 0.0070 - val_loss: 0.0381\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.0066 - val_loss: 0.0400\n",
      "Epoch 82/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0417\n",
      "Epoch 83/1000\n",
      "1s - loss: 0.0065 - val_loss: 0.0391\n",
      "Epoch 84/1000\n",
      "1s - loss: 0.0063 - val_loss: 0.0395\n",
      "Epoch 85/1000\n",
      "1s - loss: 0.0066 - val_loss: 0.0422\n",
      "Epoch 86/1000\n",
      "1s - loss: 0.0069 - val_loss: 0.0417\n",
      "Epoch 87/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0407\n",
      "Epoch 88/1000\n",
      "1s - loss: 0.0065 - val_loss: 0.0428\n",
      "Epoch 89/1000\n",
      "1s - loss: 0.0071 - val_loss: 0.0456\n",
      "Epoch 90/1000\n",
      "1s - loss: 0.0063 - val_loss: 0.0442\n",
      "Epoch 91/1000\n",
      "1s - loss: 0.0061 - val_loss: 0.0435\n",
      "Epoch 92/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0484\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.0062 - val_loss: 0.0415\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.0066 - val_loss: 0.0456\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0437\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.0064 - val_loss: 0.0442\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.0068 - val_loss: 0.0461\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.0068 - val_loss: 0.0425\n",
      "Accuracy: 0.7907725321888412\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      0.70      0.82        10\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         1\n",
      "         43       1.00      0.22      0.36        18\n",
      "         44       1.00      1.00      1.00         2\n",
      "         45       1.00      0.95      0.97        38\n",
      "         46       1.00      1.00      1.00         1\n",
      "         47       0.86      1.00      0.93        25\n",
      "         48       1.00      0.75      0.86        84\n",
      "         49       0.95      1.00      0.98        20\n",
      "         50       1.00      1.00      1.00        38\n",
      "         51       1.00      1.00      1.00         5\n",
      "         52       0.70      1.00      0.83        62\n",
      "         53       1.00      0.31      0.48        35\n",
      "         54       0.96      0.78      0.86        60\n",
      "         55       1.00      0.98      0.99        44\n",
      "         56       0.97      0.94      0.95        31\n",
      "         57       0.57      0.96      0.72        48\n",
      "         58       0.88      0.23      0.36        31\n",
      "         59       0.80      0.85      0.82        84\n",
      "         60       1.00      1.00      1.00        36\n",
      "         61       0.83      0.94      0.88        66\n",
      "         62       0.72      0.78      0.75        65\n",
      "         63       1.00      0.29      0.45        17\n",
      "         64       0.84      0.86      0.85        73\n",
      "         65       0.92      1.00      0.96        12\n",
      "         66       0.83      0.95      0.88        20\n",
      "         67       0.90      0.87      0.88        53\n",
      "         68       0.86      0.60      0.71        10\n",
      "         69       0.98      0.91      0.95        57\n",
      "         70       1.00      1.00      1.00         3\n",
      "         71       0.50      0.85      0.63        13\n",
      "         72       0.45      0.45      0.45        20\n",
      "         73       0.75      0.20      0.32        15\n",
      "         74       0.60      0.92      0.73        13\n",
      "         75       1.00      1.00      1.00         2\n",
      "         76       0.75      1.00      0.86         3\n",
      "         77       0.40      1.00      0.57         2\n",
      "         78       1.00      0.64      0.78        14\n",
      "         79       1.00      1.00      1.00         2\n",
      "         80       1.00      1.00      1.00         1\n",
      "         81       1.00      1.00      1.00         1\n",
      "         82       1.00      1.00      1.00         1\n",
      "         83       1.00      1.00      1.00         1\n",
      "         84       1.00      1.00      1.00         1\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.87      0.82      0.82      1141\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_119 (InputLayer)       (None, 1, 36, 180)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 50, 31, 178)       950       \n",
      "_________________________________________________________________\n",
      "activation_246 (Activation)  (None, 50, 31, 178)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_124 (MaxPoolin (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "dropout_229 (Dropout)        (None, 50, 5, 59)         0         \n",
      "_________________________________________________________________\n",
      "flatten_119 (Flatten)        (None, 14750)             0         \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 256)               3776256   \n",
      "_________________________________________________________________\n",
      "activation_247 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_230 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 49)                12593     \n",
      "=================================================================\n",
      "Total params: 3,789,799.0\n",
      "Trainable params: 3,789,799.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 4236 samples, validate on 863 samples\n",
      "Epoch 1/1000\n",
      "1s - loss: 0.2486 - val_loss: 0.0768\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.1151 - val_loss: 0.0490\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.0810 - val_loss: 0.0328\n",
      "Epoch 4/1000\n",
      "1s - loss: 0.0622 - val_loss: 0.0238\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.0500 - val_loss: 0.0188\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.0424 - val_loss: 0.0152\n",
      "Epoch 7/1000\n",
      "1s - loss: 0.0373 - val_loss: 0.0135\n",
      "Epoch 8/1000\n",
      "1s - loss: 0.0337 - val_loss: 0.0128\n",
      "Epoch 9/1000\n",
      "1s - loss: 0.0304 - val_loss: 0.0115\n",
      "Epoch 10/1000\n",
      "1s - loss: 0.0280 - val_loss: 0.0104\n",
      "Epoch 11/1000\n",
      "1s - loss: 0.0269 - val_loss: 0.0103\n",
      "Epoch 12/1000\n",
      "1s - loss: 0.0240 - val_loss: 0.0096\n",
      "Epoch 13/1000\n",
      "1s - loss: 0.0229 - val_loss: 0.0087\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.0213 - val_loss: 0.0092\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.0210 - val_loss: 0.0080\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.0201 - val_loss: 0.0087\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.0187 - val_loss: 0.0079\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.0176 - val_loss: 0.0072\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.0171 - val_loss: 0.0078\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.0165 - val_loss: 0.0067\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.0154 - val_loss: 0.0072\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.0149 - val_loss: 0.0070\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.0141 - val_loss: 0.0076\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.0145 - val_loss: 0.0067\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.0135 - val_loss: 0.0072\n",
      "Epoch 26/1000\n",
      "1s - loss: 0.0137 - val_loss: 0.0063\n",
      "Epoch 27/1000\n",
      "1s - loss: 0.0137 - val_loss: 0.0071\n",
      "Epoch 28/1000\n",
      "1s - loss: 0.0125 - val_loss: 0.0079\n",
      "Epoch 29/1000\n",
      "1s - loss: 0.0122 - val_loss: 0.0072\n",
      "Epoch 30/1000\n",
      "1s - loss: 0.0120 - val_loss: 0.0061\n",
      "Epoch 31/1000\n",
      "1s - loss: 0.0116 - val_loss: 0.0070\n",
      "Epoch 32/1000\n",
      "1s - loss: 0.0119 - val_loss: 0.0068\n",
      "Epoch 33/1000\n",
      "1s - loss: 0.0110 - val_loss: 0.0063\n",
      "Epoch 34/1000\n",
      "1s - loss: 0.0108 - val_loss: 0.0073\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.0105 - val_loss: 0.0067\n",
      "Epoch 36/1000\n",
      "1s - loss: 0.0099 - val_loss: 0.0071\n",
      "Epoch 37/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-23fcc5ca3abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m                                \u001b[0mdropout_conv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_dense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproba_threshold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                \u001b[0mn_filters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                                verbose=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-23fcc5ca3abd>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(folds, create_model, dropout_conv, dropout_dense, proba_threshold, n_filters, filter_size, pool_size, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m                             ],\n\u001b[1;32m     30\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_verbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                  )\n\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(folds, create_model,\n",
    "                       dropout_conv, dropout_dense, proba_threshold,\n",
    "                       n_filters, filter_size, pool_size,\n",
    "                       verbose=False):\n",
    "    y_test_all_folds = None\n",
    "    y_test_predicted_all_folds = None\n",
    "    for i, (list_of_X_train, y_train, list_of_X_test, y_test) in enumerate(folds):\n",
    "        if verbose:\n",
    "            fit_verbose = 2\n",
    "            validation_data = (list_of_X_test, y_test)\n",
    "        else:\n",
    "            fit_verbose = 0\n",
    "            validation_data = None\n",
    "        \n",
    "        # model_dir = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        # os.mkdir(model_dir)\n",
    "\n",
    "        model = create_model(list_of_X_train, max_pitch - min_pitch + 1,\n",
    "                             dropout_conv, dropout_dense,\n",
    "                             n_filters, filter_size, pool_size)\n",
    "        model.fit(list_of_X_train, y_train,\n",
    "                  epochs=1000,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  sample_weight=None,\n",
    "                  class_weight=None,\n",
    "                  callbacks=[EarlyStopping(monitor='loss', patience=6),\n",
    "                             # ModelCheckpoint(os.path.join(model_dir, 'model.' + str(i) + '.{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                             #                 monitor='val_loss', save_best_only=True)\n",
    "                            ],\n",
    "                  verbose=fit_verbose,\n",
    "                  validation_data=validation_data,\n",
    "                 )\n",
    "\n",
    "        # Load model with lowest val_loss\n",
    "        # path_to_model = max([os.path.join(model_dir, file)\n",
    "        #                      for file in os.listdir(model_dir)\n",
    "        #                      if file.startswith('model.' + str(i) + '.')])\n",
    "        # print(path_to_model)\n",
    "        # model = load_model(path_to_model)\n",
    "\n",
    "        y_test_predicted = predict(model, proba_threshold, list_of_X_test, y_test)\n",
    "        if verbose:\n",
    "            print_metrics(y_test, y_test_predicted)\n",
    "\n",
    "        if y_test_all_folds is None:\n",
    "            y_test_all_folds = y_test\n",
    "        else:\n",
    "            y_test_all_folds = np.concatenate((y_test_all_folds, y_test))\n",
    "\n",
    "        if y_test_predicted_all_folds is None:\n",
    "            y_test_predicted_all_folds = y_test_predicted\n",
    "        else:\n",
    "            y_test_predicted_all_folds = np.concatenate((y_test_predicted_all_folds, y_test_predicted))\n",
    "\n",
    "    print_metrics(y_test_all_folds, y_test_predicted_all_folds)\n",
    "\n",
    "models = [\n",
    "    ('model_5', create_model_5),\n",
    "]\n",
    "dropout_conv = 0.15\n",
    "dropout_dense = 0.3\n",
    "proba_threshold = 0.5\n",
    "for model_name, create_model in models:\n",
    "    print(model_name)\n",
    "    for n_filters in [10]:\n",
    "        for filter_size, pool_size in [((10, 3), (3, 3))]:\n",
    "            print('dropout_conv={}, dropout_dense={}, proba_threshold={}, n_filters={}, filter_size={}, pool_size={}'.format(\n",
    "                dropout_conv, dropout_dense, proba_threshold, n_filters, filter_size, pool_size\n",
    "            ))\n",
    "            train_and_evaluate(folds, create_model,\n",
    "                               dropout_conv, dropout_dense, proba_threshold,\n",
    "                               n_filters, filter_size, pool_size,\n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_120 (InputLayer)       (None, 1, 36, 180)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 10, 27, 178)       310       \n",
      "_________________________________________________________________\n",
      "activation_248 (Activation)  (None, 10, 27, 178)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_125 (MaxPoolin (None, 10, 9, 59)         0         \n",
      "_________________________________________________________________\n",
      "dropout_231 (Dropout)        (None, 10, 9, 59)         0         \n",
      "_________________________________________________________________\n",
      "flatten_120 (Flatten)        (None, 5310)              0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 256)               1359616   \n",
      "_________________________________________________________________\n",
      "activation_249 (Activation)  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_232 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 49)                12593     \n",
      "=================================================================\n",
      "Total params: 1,372,519.0\n",
      "Trainable params: 1,372,519.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "Train on 4466 samples, validate on 633 samples\n",
      "Epoch 1/1000\n",
      "0s - loss: 0.2343 - val_loss: 0.0793\n",
      "Epoch 2/1000\n",
      "0s - loss: 0.1047 - val_loss: 0.0495\n",
      "Epoch 3/1000\n",
      "0s - loss: 0.0707 - val_loss: 0.0364\n",
      "Epoch 4/1000\n",
      "0s - loss: 0.0546 - val_loss: 0.0283\n",
      "Epoch 5/1000\n",
      "0s - loss: 0.0431 - val_loss: 0.0232\n",
      "Epoch 6/1000\n",
      "0s - loss: 0.0356 - val_loss: 0.0196\n",
      "Epoch 7/1000\n",
      "0s - loss: 0.0300 - val_loss: 0.0175\n",
      "Epoch 8/1000\n",
      "0s - loss: 0.0257 - val_loss: 0.0166\n",
      "Epoch 9/1000\n",
      "0s - loss: 0.0230 - val_loss: 0.0141\n",
      "Epoch 10/1000\n",
      "0s - loss: 0.0206 - val_loss: 0.0141\n",
      "Epoch 11/1000\n",
      "0s - loss: 0.0187 - val_loss: 0.0130\n",
      "Epoch 12/1000\n",
      "0s - loss: 0.0174 - val_loss: 0.0125\n",
      "Epoch 13/1000\n",
      "0s - loss: 0.0160 - val_loss: 0.0125\n",
      "Epoch 14/1000\n",
      "0s - loss: 0.0145 - val_loss: 0.0120\n",
      "Epoch 15/1000\n",
      "0s - loss: 0.0136 - val_loss: 0.0115\n",
      "Epoch 16/1000\n",
      "0s - loss: 0.0124 - val_loss: 0.0101\n",
      "Epoch 17/1000\n",
      "0s - loss: 0.0118 - val_loss: 0.0107\n",
      "Epoch 18/1000\n",
      "0s - loss: 0.0109 - val_loss: 0.0108\n",
      "Epoch 19/1000\n",
      "0s - loss: 0.0102 - val_loss: 0.0107\n",
      "Epoch 20/1000\n",
      "0s - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 21/1000\n",
      "0s - loss: 0.0092 - val_loss: 0.0104\n",
      "Epoch 22/1000\n",
      "0s - loss: 0.0090 - val_loss: 0.0111\n",
      "Epoch 23/1000\n",
      "0s - loss: 0.0086 - val_loss: 0.0098\n",
      "Epoch 24/1000\n",
      "0s - loss: 0.0079 - val_loss: 0.0108\n",
      "Epoch 25/1000\n",
      "0s - loss: 0.0075 - val_loss: 0.0110\n",
      "Epoch 26/1000\n",
      "0s - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 27/1000\n",
      "0s - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 28/1000\n",
      "0s - loss: 0.0065 - val_loss: 0.0104\n",
      "Epoch 29/1000\n",
      "0s - loss: 0.0064 - val_loss: 0.0103\n",
      "Epoch 30/1000\n",
      "0s - loss: 0.0061 - val_loss: 0.0107\n",
      "Epoch 31/1000\n",
      "0s - loss: 0.0063 - val_loss: 0.0112\n",
      "Epoch 32/1000\n",
      "0s - loss: 0.0060 - val_loss: 0.0114\n",
      "Epoch 33/1000\n",
      "0s - loss: 0.0055 - val_loss: 0.0111\n",
      "Epoch 34/1000\n",
      "0s - loss: 0.0052 - val_loss: 0.0116\n",
      "Epoch 35/1000\n",
      "0s - loss: 0.0049 - val_loss: 0.0110\n",
      "Epoch 36/1000\n",
      "0s - loss: 0.0047 - val_loss: 0.0111\n",
      "Epoch 37/1000\n",
      "0s - loss: 0.0047 - val_loss: 0.0121\n",
      "Epoch 38/1000\n",
      "0s - loss: 0.0046 - val_loss: 0.0121\n",
      "Epoch 39/1000\n",
      "0s - loss: 0.0045 - val_loss: 0.0112\n",
      "Epoch 40/1000\n",
      "0s - loss: 0.0044 - val_loss: 0.0113\n",
      "Epoch 41/1000\n",
      "0s - loss: 0.0043 - val_loss: 0.0112\n",
      "Epoch 42/1000\n",
      "0s - loss: 0.0041 - val_loss: 0.0106\n",
      "Epoch 43/1000\n",
      "0s - loss: 0.0042 - val_loss: 0.0105\n",
      "Epoch 44/1000\n",
      "0s - loss: 0.0042 - val_loss: 0.0113\n",
      "Epoch 45/1000\n",
      "0s - loss: 0.0038 - val_loss: 0.0117\n",
      "Epoch 46/1000\n",
      "0s - loss: 0.0039 - val_loss: 0.0113\n",
      "Epoch 47/1000\n",
      "0s - loss: 0.0036 - val_loss: 0.0122\n",
      "Epoch 48/1000\n",
      "0s - loss: 0.0035 - val_loss: 0.0121\n",
      "Epoch 49/1000\n",
      "0s - loss: 0.0035 - val_loss: 0.0119\n",
      "Epoch 50/1000\n",
      "0s - loss: 0.0034 - val_loss: 0.0112\n",
      "Epoch 51/1000\n",
      "0s - loss: 0.0034 - val_loss: 0.0116\n",
      "Epoch 52/1000\n",
      "0s - loss: 0.0033 - val_loss: 0.0124\n",
      "Epoch 53/1000\n",
      "0s - loss: 0.0032 - val_loss: 0.0123\n",
      "Epoch 54/1000\n",
      "0s - loss: 0.0030 - val_loss: 0.0117\n",
      "Epoch 55/1000\n",
      "0s - loss: 0.0031 - val_loss: 0.0132\n",
      "Epoch 56/1000\n",
      "0s - loss: 0.0031 - val_loss: 0.0114\n",
      "Epoch 57/1000\n",
      "0s - loss: 0.0030 - val_loss: 0.0116\n",
      "Epoch 58/1000\n",
      "0s - loss: 0.0027 - val_loss: 0.0116\n",
      "Epoch 59/1000\n",
      "0s - loss: 0.0028 - val_loss: 0.0121\n",
      "Epoch 60/1000\n",
      "0s - loss: 0.0028 - val_loss: 0.0113\n",
      "Epoch 61/1000\n",
      "0s - loss: 0.0027 - val_loss: 0.0127\n",
      "Epoch 62/1000\n",
      "0s - loss: 0.0028 - val_loss: 0.0114\n",
      "Epoch 63/1000\n",
      "0s - loss: 0.0030 - val_loss: 0.0119\n",
      "Epoch 64/1000\n",
      "0s - loss: 0.0028 - val_loss: 0.0122\n",
      "Epoch 65/1000\n",
      "0s - loss: 0.0025 - val_loss: 0.0131\n",
      "Epoch 66/1000\n",
      "0s - loss: 0.0024 - val_loss: 0.0120\n",
      "Epoch 67/1000\n",
      "0s - loss: 0.0026 - val_loss: 0.0130\n",
      "Epoch 68/1000\n",
      "0s - loss: 0.0027 - val_loss: 0.0128\n",
      "Epoch 69/1000\n",
      "0s - loss: 0.0025 - val_loss: 0.0126\n",
      "Epoch 70/1000\n",
      "0s - loss: 0.0027 - val_loss: 0.0129\n",
      "Epoch 71/1000\n",
      "0s - loss: 0.0026 - val_loss: 0.0125\n",
      "Epoch 72/1000\n",
      "0s - loss: 0.0026 - val_loss: 0.0131\n",
      "Epoch 73/1000\n",
      "0s - loss: 0.0026 - val_loss: 0.0126\n",
      "0.1\n",
      "Accuracy: 0.9399684044233807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        13\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         5\n",
      "         44       1.00      1.00      1.00         4\n",
      "         45       1.00      1.00      1.00        29\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      1.00      1.00        21\n",
      "         48       1.00      0.98      0.99        59\n",
      "         49       1.00      1.00      1.00        15\n",
      "         50       1.00      1.00      1.00        43\n",
      "         51       1.00      1.00      1.00         6\n",
      "         52       1.00      1.00      1.00        52\n",
      "         53       0.95      1.00      0.97        19\n",
      "         54       1.00      0.94      0.97        32\n",
      "         55       0.96      0.98      0.97        45\n",
      "         56       1.00      0.97      0.99        38\n",
      "         57       0.98      0.98      0.98        45\n",
      "         58       0.80      0.80      0.80         5\n",
      "         59       0.99      1.00      0.99        73\n",
      "         60       0.95      0.95      0.95        39\n",
      "         61       1.00      0.91      0.95        46\n",
      "         62       0.88      0.93      0.90        45\n",
      "         63       1.00      0.88      0.93         8\n",
      "         64       0.97      0.97      0.97        63\n",
      "         65       1.00      0.93      0.97        15\n",
      "         66       1.00      1.00      1.00        14\n",
      "         67       1.00      0.78      0.88        18\n",
      "         68       1.00      0.62      0.77         8\n",
      "         69       0.95      0.97      0.96        39\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       0.75      0.60      0.67         5\n",
      "         72       1.00      0.70      0.82        10\n",
      "         73       1.00      0.50      0.67         4\n",
      "         74       1.00      0.83      0.91        12\n",
      "         75       1.00      1.00      1.00         3\n",
      "         76       1.00      1.00      1.00         4\n",
      "         77       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         2\n",
      "         79       1.00      1.00      1.00         3\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       1.00      1.00      1.00         3\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.98      0.95      0.96       862\n",
      "\n",
      "0.2\n",
      "Accuracy: 0.9462875197472354\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        13\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         5\n",
      "         44       1.00      1.00      1.00         4\n",
      "         45       1.00      1.00      1.00        29\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      1.00      1.00        21\n",
      "         48       1.00      0.98      0.99        59\n",
      "         49       1.00      1.00      1.00        15\n",
      "         50       1.00      1.00      1.00        43\n",
      "         51       1.00      1.00      1.00         6\n",
      "         52       1.00      1.00      1.00        52\n",
      "         53       0.95      0.95      0.95        19\n",
      "         54       1.00      0.94      0.97        32\n",
      "         55       0.96      0.98      0.97        45\n",
      "         56       1.00      0.97      0.99        38\n",
      "         57       0.98      0.98      0.98        45\n",
      "         58       0.80      0.80      0.80         5\n",
      "         59       1.00      1.00      1.00        73\n",
      "         60       0.95      0.92      0.94        39\n",
      "         61       1.00      0.91      0.95        46\n",
      "         62       0.91      0.91      0.91        45\n",
      "         63       1.00      0.88      0.93         8\n",
      "         64       1.00      0.97      0.98        63\n",
      "         65       1.00      0.93      0.97        15\n",
      "         66       1.00      1.00      1.00        14\n",
      "         67       1.00      0.78      0.88        18\n",
      "         68       1.00      0.62      0.77         8\n",
      "         69       0.97      0.97      0.97        39\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       0.75      0.60      0.67         5\n",
      "         72       1.00      0.70      0.82        10\n",
      "         73       1.00      0.50      0.67         4\n",
      "         74       1.00      0.83      0.91        12\n",
      "         75       1.00      1.00      1.00         3\n",
      "         76       1.00      1.00      1.00         4\n",
      "         77       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         2\n",
      "         79       1.00      1.00      1.00         3\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       1.00      1.00      1.00         3\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.98      0.95      0.97       862\n",
      "\n",
      "0.3\n",
      "Accuracy: 0.9415481832543444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        13\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         5\n",
      "         44       1.00      1.00      1.00         4\n",
      "         45       1.00      1.00      1.00        29\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      1.00      1.00        21\n",
      "         48       1.00      0.98      0.99        59\n",
      "         49       1.00      1.00      1.00        15\n",
      "         50       1.00      1.00      1.00        43\n",
      "         51       1.00      1.00      1.00         6\n",
      "         52       1.00      1.00      1.00        52\n",
      "         53       0.95      0.95      0.95        19\n",
      "         54       1.00      0.94      0.97        32\n",
      "         55       0.96      0.98      0.97        45\n",
      "         56       1.00      0.97      0.99        38\n",
      "         57       0.98      0.98      0.98        45\n",
      "         58       0.80      0.80      0.80         5\n",
      "         59       1.00      0.99      0.99        73\n",
      "         60       0.95      0.92      0.94        39\n",
      "         61       1.00      0.91      0.95        46\n",
      "         62       0.91      0.89      0.90        45\n",
      "         63       1.00      0.88      0.93         8\n",
      "         64       1.00      0.97      0.98        63\n",
      "         65       1.00      0.93      0.97        15\n",
      "         66       1.00      1.00      1.00        14\n",
      "         67       1.00      0.72      0.84        18\n",
      "         68       1.00      0.62      0.77         8\n",
      "         69       0.97      0.95      0.96        39\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       0.75      0.60      0.67         5\n",
      "         72       1.00      0.70      0.82        10\n",
      "         73       1.00      0.50      0.67         4\n",
      "         74       1.00      0.83      0.91        12\n",
      "         75       1.00      1.00      1.00         3\n",
      "         76       1.00      1.00      1.00         4\n",
      "         77       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         2\n",
      "         79       1.00      1.00      1.00         3\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       1.00      1.00      1.00         3\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.98      0.95      0.96       862\n",
      "\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9415481832543444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        13\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         5\n",
      "         44       1.00      1.00      1.00         4\n",
      "         45       1.00      1.00      1.00        29\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      1.00      1.00        21\n",
      "         48       1.00      0.98      0.99        59\n",
      "         49       1.00      1.00      1.00        15\n",
      "         50       1.00      1.00      1.00        43\n",
      "         51       1.00      1.00      1.00         6\n",
      "         52       1.00      1.00      1.00        52\n",
      "         53       0.95      0.95      0.95        19\n",
      "         54       1.00      0.94      0.97        32\n",
      "         55       0.96      0.98      0.97        45\n",
      "         56       1.00      0.97      0.99        38\n",
      "         57       0.98      0.98      0.98        45\n",
      "         58       0.80      0.80      0.80         5\n",
      "         59       1.00      0.99      0.99        73\n",
      "         60       0.95      0.92      0.94        39\n",
      "         61       1.00      0.91      0.95        46\n",
      "         62       0.91      0.89      0.90        45\n",
      "         63       1.00      0.88      0.93         8\n",
      "         64       1.00      0.97      0.98        63\n",
      "         65       1.00      0.93      0.97        15\n",
      "         66       1.00      1.00      1.00        14\n",
      "         67       1.00      0.72      0.84        18\n",
      "         68       1.00      0.62      0.77         8\n",
      "         69       0.97      0.92      0.95        39\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      0.60      0.75         5\n",
      "         72       1.00      0.70      0.82        10\n",
      "         73       1.00      0.50      0.67         4\n",
      "         74       1.00      0.83      0.91        12\n",
      "         75       1.00      1.00      1.00         3\n",
      "         76       1.00      1.00      1.00         4\n",
      "         77       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         2\n",
      "         79       1.00      1.00      1.00         3\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       1.00      1.00      1.00         3\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.99      0.95      0.96       862\n",
      "\n",
      "0.5\n",
      "Accuracy: 0.9383886255924171\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        13\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         5\n",
      "         44       1.00      1.00      1.00         4\n",
      "         45       1.00      1.00      1.00        29\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      0.95      0.98        21\n",
      "         48       1.00      0.98      0.99        59\n",
      "         49       1.00      1.00      1.00        15\n",
      "         50       1.00      1.00      1.00        43\n",
      "         51       1.00      1.00      1.00         6\n",
      "         52       1.00      1.00      1.00        52\n",
      "         53       0.95      0.95      0.95        19\n",
      "         54       1.00      0.94      0.97        32\n",
      "         55       0.96      0.98      0.97        45\n",
      "         56       1.00      0.97      0.99        38\n",
      "         57       0.98      0.98      0.98        45\n",
      "         58       0.80      0.80      0.80         5\n",
      "         59       1.00      0.99      0.99        73\n",
      "         60       0.95      0.92      0.94        39\n",
      "         61       1.00      0.91      0.95        46\n",
      "         62       0.91      0.89      0.90        45\n",
      "         63       1.00      0.88      0.93         8\n",
      "         64       1.00      0.97      0.98        63\n",
      "         65       1.00      0.93      0.97        15\n",
      "         66       1.00      1.00      1.00        14\n",
      "         67       1.00      0.67      0.80        18\n",
      "         68       1.00      0.62      0.77         8\n",
      "         69       0.97      0.92      0.95        39\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      0.60      0.75         5\n",
      "         72       1.00      0.70      0.82        10\n",
      "         73       1.00      0.50      0.67         4\n",
      "         74       1.00      0.75      0.86        12\n",
      "         75       1.00      1.00      1.00         3\n",
      "         76       1.00      1.00      1.00         4\n",
      "         77       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         2\n",
      "         79       1.00      1.00      1.00         3\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       1.00      1.00      1.00         3\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.99      0.94      0.96       862\n",
      "\n",
      "0.6\n",
      "Accuracy: 0.9383886255924171\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      1.00      1.00        13\n",
      "         41       1.00      1.00      1.00         3\n",
      "         42       1.00      1.00      1.00         2\n",
      "         43       1.00      1.00      1.00         5\n",
      "         44       1.00      1.00      1.00         4\n",
      "         45       1.00      1.00      1.00        29\n",
      "         46       1.00      1.00      1.00         3\n",
      "         47       1.00      0.95      0.98        21\n",
      "         48       1.00      0.98      0.99        59\n",
      "         49       1.00      1.00      1.00        15\n",
      "         50       1.00      1.00      1.00        43\n",
      "         51       1.00      1.00      1.00         6\n",
      "         52       1.00      1.00      1.00        52\n",
      "         53       0.95      0.95      0.95        19\n",
      "         54       1.00      0.94      0.97        32\n",
      "         55       0.96      0.98      0.97        45\n",
      "         56       1.00      0.97      0.99        38\n",
      "         57       0.98      0.98      0.98        45\n",
      "         58       0.80      0.80      0.80         5\n",
      "         59       1.00      0.99      0.99        73\n",
      "         60       0.95      0.92      0.94        39\n",
      "         61       1.00      0.91      0.95        46\n",
      "         62       0.91      0.87      0.89        45\n",
      "         63       1.00      0.88      0.93         8\n",
      "         64       1.00      0.97      0.98        63\n",
      "         65       1.00      0.80      0.89        15\n",
      "         66       1.00      1.00      1.00        14\n",
      "         67       1.00      0.67      0.80        18\n",
      "         68       1.00      0.62      0.77         8\n",
      "         69       0.97      0.92      0.95        39\n",
      "         70       1.00      0.83      0.91         6\n",
      "         71       1.00      0.60      0.75         5\n",
      "         72       1.00      0.70      0.82        10\n",
      "         73       1.00      0.50      0.67         4\n",
      "         74       1.00      0.75      0.86        12\n",
      "         75       1.00      1.00      1.00         3\n",
      "         76       1.00      1.00      1.00         4\n",
      "         77       1.00      1.00      1.00         1\n",
      "         78       1.00      1.00      1.00         2\n",
      "         79       1.00      1.00      1.00         3\n",
      "         80       0.00      0.00      0.00         0\n",
      "         81       0.00      0.00      0.00         0\n",
      "         82       0.00      0.00      0.00         0\n",
      "         83       1.00      1.00      1.00         3\n",
      "         84       0.00      0.00      0.00         0\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.99      0.94      0.96       862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_X_train, y_train, list_of_X_test, y_test = folds[0]\n",
    "fit_verbose = 2\n",
    "validation_data = (list_of_X_test, y_test)\n",
    "\n",
    "model = create_model_5(list_of_X_train, max_pitch - min_pitch + 1,\n",
    "                       0.15, 0.3, 10, (10, 3), (3, 3))\n",
    "model.fit(list_of_X_train, y_train,\n",
    "          epochs=1000,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          sample_weight=None,\n",
    "          class_weight=None,\n",
    "          callbacks=[EarlyStopping(monitor='loss', patience=6),\n",
    "                     # ModelCheckpoint(os.path.join(model_dir, 'model.' + str(i) + '.{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                     #                 monitor='val_loss', save_best_only=True)\n",
    "                    ],\n",
    "          verbose=fit_verbose,\n",
    "          validation_data=validation_data,\n",
    "         )\n",
    "\n",
    "for proba_threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    print(proba_threshold)\n",
    "    y_test_predicted = predict(model, proba_threshold, list_of_X_test, y_test)\n",
    "    print_metrics(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy.ma as ma\n",
    "\n",
    "def nice_imshow(ax, data, vmin=None, vmax=None, cmap=None):\n",
    "    \"\"\"Wrapper around pl.imshow\"\"\"\n",
    "    if cmap is None:\n",
    "        cmap = cm.jet\n",
    "    if vmin is None:\n",
    "        vmin = data.min()\n",
    "    if vmax is None:\n",
    "        vmax = data.max()\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    im = ax.imshow(data, vmin=vmin, vmax=vmax, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "def make_mosaic(imgs, nrows, ncols, border=1):\n",
    "    \"\"\"\n",
    "    Given a set of images with all the same shape, makes a\n",
    "    mosaic with nrows and ncols\n",
    "    \"\"\"\n",
    "    nimgs = imgs.shape[0]\n",
    "    imshape = imgs.shape[1:]\n",
    "    \n",
    "    mosaic = ma.masked_all((nrows * imshape[0] + (nrows - 1) * border,\n",
    "                            ncols * imshape[1] + (ncols - 1) * border),\n",
    "                            dtype=np.float32)\n",
    "    \n",
    "    paddedh = imshape[0] + border\n",
    "    paddedw = imshape[1] + border\n",
    "    for i in range(nimgs):\n",
    "        row = int(np.floor(i / ncols))\n",
    "        col = i % ncols\n",
    "        \n",
    "        mosaic[row * paddedh:row * paddedh + imshape[0],\n",
    "               col * paddedw:col * paddedw + imshape[1]] = imgs[i]\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_129\n"
     ]
    }
   ],
   "source": [
    "layer = model.get_layer(index=1)\n",
    "print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 1, 10)\n",
      "(10, 10, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAATGCAYAAAA2Wl0TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAg6bAAgAAAGDQ/gOagXSshqDhdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5af484cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize weights\n",
    "W = layer.get_weights()[0]\n",
    "print(W.shape)\n",
    "# (7, 3, 1, 10) -> (10, 3, 7)\n",
    "W = np.squeeze(W).T\n",
    "# (10, 3, 7) -> (10, 7, 3)\n",
    "W_filters_transposed = np.empty((W.shape[0], W.shape[2], W.shape[1]))\n",
    "for i, array in enumerate(W):\n",
    "    W_filters_transposed[i] = array.T\n",
    "print(W_filters_transposed.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.title('conv1 weights')\n",
    "nice_imshow(plt.gca(), make_mosaic(W_filters_transposed, 2, 5), cmap=cm.binary)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
