{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Activation, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Input, Model, load_model, model_from_json\n",
    "from librosa import cqt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from warnings import warn\n",
    "from zipfile import ZipFile\n",
    "\n",
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from music_transcription.pitch_detection.cnn_cqt_pitch_detection import CnnCqtFeatureExtractor\n",
    "from music_transcription.pitch_detection.read_data import get_wav_and_truth_files, read_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS_CV = {1, 2}\n",
    "DATASETS_ADDITIONAL = {3, 9, 10, 11}\n",
    "\n",
    "sample_rate = 44100\n",
    "subsampling_step = 1\n",
    "min_pitch = 40\n",
    "max_pitch = 88\n",
    "onset_group_threshold_seconds = 0.05\n",
    "\n",
    "image_data_format = 'channels_first'\n",
    "cqt_configs = [\n",
    "    {\n",
    "        'hop_length': 512,\n",
    "        'fmin': 55.0,\n",
    "        'n_bins': 180,\n",
    "        'bins_per_octave': 36,\n",
    "        'scale': False,\n",
    "    },\n",
    "]\n",
    "\n",
    "LOSS = 'binary_crossentropy'\n",
    "OPTIMIZER = 'adam'\n",
    "METRICS = None\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping AR_Lick11_FN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping AR_Lick11_KN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping AR_Lick11_MN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:135: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\audio\\desktop.ini, not a .wav file.\n",
      "  warn('Skipping ' + path_to_wav + ', not a .wav file.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping FS_Lick11_FN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping FS_Lick11_KN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping FS_Lick11_MN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping LP_Lick11_FN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping LP_Lick11_KN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping LP_Lick11_MN.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G63-44104-1111-20675.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G71-40100-1111-20749.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G83-45105-1111-20988.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G91-43103-1111-21064.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping G93-46106-1111-21145.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping P94-43120-1111-41410.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\onset_detection\\read_data.py:133: UserWarning: Skipping P94-44110-1111-41396.wav, no truth found.\n",
      "  warn('Skipping ' + wav_file + ', no truth found.')\n"
     ]
    }
   ],
   "source": [
    "wav_file_paths_cv, truth_dataset_format_tuples_cv = get_wav_and_truth_files(DATASETS_CV)\n",
    "wav_file_paths_additional, truth_dataset_format_tuples_additional = get_wav_and_truth_files(DATASETS_ADDITIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\AR_NH_IV.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\AR_NH_IX.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\FS_NH_IV.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\FS_NH_IX.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\LP_NH_IV.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:56: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset3\\audio\\pathetique_poly.wav, cannot handle stereo signal.\n",
      "  warn('Skipping ' + path_to_wav + ', cannot handle stereo signal.')\n",
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset3\\annotation\\pathetique_poly.xml, pitch 30 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(515965, 180)\n",
      "3.63677319949\n",
      "22.0932928692\n",
      "Standardizing for each spectrogram and bin\n",
      "-2.02342757837e-16\n",
      "1.0\n",
      "(4466, 16, 180)\n",
      "Reshaping data\n",
      "(4466, 1, 16, 180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michel\\FH\\IP6\\git\\music_transcription\\pitch_detection\\read_data.py:90: UserWarning: Skipping ..\\data\\IDMT-SMT-GUITAR_V2\\dataset2\\annotation\\LP_NH_IX.xml, pitch 92 is out of range.\n",
      "  warn('Skipping {}, pitch {} is out of range.'.format(path_to_xml, pitch))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spectrograms\n",
      "(73188, 180)\n",
      "5.06737530463\n",
      "27.5702815749\n",
      "Standardizing for each spectrogram and bin\n",
      "0.0824521316465\n",
      "1.19121556558\n",
      "(633, 16, 180)\n",
      "Reshaping data\n",
      "(633, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(527000, 180)\n",
      "3.56672113525\n",
      "21.6869298092\n",
      "Standardizing for each spectrogram and bin\n",
      "-2.03703898456e-16\n",
      "1.0\n",
      "(4404, 16, 180)\n",
      "Reshaping data\n",
      "(4404, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "(62153, 180)\n",
      "5.91534937004\n",
      "30.9263472148\n",
      "Standardizing for each spectrogram and bin\n",
      "0.149010008662\n",
      "1.42025840175\n",
      "(695, 16, 180)\n",
      "Reshaping data\n",
      "(695, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(504083, 180)\n",
      "3.68388396785\n",
      "22.797992656\n",
      "Standardizing for each spectrogram and bin\n",
      "1.54527309082e-16\n",
      "1.0\n",
      "(4167, 16, 180)\n",
      "Reshaping data\n",
      "(4167, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "(85070, 180)\n",
      "4.58840326211\n",
      "23.1412538061\n",
      "Standardizing for each spectrogram and bin\n",
      "0.0728228072821\n",
      "1.1014277565\n",
      "(932, 16, 180)\n",
      "Reshaping data\n",
      "(932, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(510210, 180)\n",
      "3.43382394529\n",
      "21.3277811016\n",
      "Standardizing for each spectrogram and bin\n",
      "1.65859997357e-16\n",
      "1.0\n",
      "(4236, 16, 180)\n",
      "Reshaping data\n",
      "(4236, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "(78943, 180)\n",
      "6.274748015\n",
      "30.8186584223\n",
      "Standardizing for each spectrogram and bin\n",
      "0.18061681751\n",
      "1.4366928998\n",
      "(863, 16, 180)\n",
      "Reshaping data\n",
      "(863, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "Fitting standard scalers for each spectrogram and bin\n",
      "(514789, 180)\n",
      "3.66276036624\n",
      "22.5576229671\n",
      "Standardizing for each spectrogram and bin\n",
      "-2.75806647857e-16\n",
      "1.0\n",
      "(4387, 16, 180)\n",
      "Reshaping data\n",
      "(4387, 1, 16, 180)\n",
      "Creating spectrograms\n",
      "(74364, 180)\n",
      "4.8648539817\n",
      "24.7547277884\n",
      "Standardizing for each spectrogram and bin\n",
      "0.0572328367426\n",
      "1.01383766925\n",
      "(712, 16, 180)\n",
      "Reshaping data\n",
      "(712, 1, 16, 180)\n"
     ]
    }
   ],
   "source": [
    "folds = []\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for k, (train_indices, test_indices) in enumerate(k_fold.split(wav_file_paths_cv)):\n",
    "    # if k > 0:\n",
    "    #     print('Skipping split {}'.format(k))\n",
    "    #     continue\n",
    "    \n",
    "    wav_file_paths_train = [wav_file_paths_cv[i] for i in train_indices] + wav_file_paths_additional\n",
    "    truth_dataset_format_tuples_train = [truth_dataset_format_tuples_cv[i] for i in train_indices] + truth_dataset_format_tuples_additional\n",
    "    wav_file_paths_test = [wav_file_paths_cv[i] for i in test_indices]\n",
    "    truth_dataset_format_tuples_test = [truth_dataset_format_tuples_cv[i] for i in test_indices]\n",
    "    \n",
    "    data_train, y_train, wav_file_paths_train_valid, truth_dataset_format_tuples_train_valid = read_data_y(\n",
    "        wav_file_paths_train, truth_dataset_format_tuples_train,\n",
    "        sample_rate, subsampling_step,\n",
    "        min_pitch, max_pitch,\n",
    "        onset_group_threshold_seconds=onset_group_threshold_seconds\n",
    "    )\n",
    "    \n",
    "    feature_extractor = CnnCqtFeatureExtractor(image_data_format, sample_rate, cqt_configs)\n",
    "    list_of_X_train, sample_file_indexes_train = feature_extractor.fit_transform(data_train)\n",
    "\n",
    "    data_test, y_test, wav_file_paths_test_valid, truth_dataset_format_tuples_test_valid = read_data_y(\n",
    "        wav_file_paths_test, truth_dataset_format_tuples_test,\n",
    "        sample_rate, subsampling_step,\n",
    "        min_pitch, max_pitch,\n",
    "        onset_group_threshold_seconds=onset_group_threshold_seconds\n",
    "    )\n",
    "    list_of_X_test, sample_file_indexes_test = feature_extractor.transform(data_test, verbose=True)\n",
    "\n",
    "    # if self.config['sample_weights'] == 'balanced':\n",
    "        # validation_data = (list_of_X_test, y_test, self._get_sample_weights(sample_file_indexes_test,\n",
    "        #                                                                     truth_dataset_format_tuples_test_valid))\n",
    "    # else:\n",
    "    \n",
    "    folds.append((list_of_X_train, y_train, list_of_X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 400, 2: 252})\n",
      "0\n",
      "defaultdict(<class 'int'>, {1: 83, 2: 48})\n",
      "1\n",
      "defaultdict(<class 'int'>, {1: 89, 2: 42})\n",
      "2\n",
      "defaultdict(<class 'int'>, {1: 77, 2: 53})\n",
      "3\n",
      "defaultdict(<class 'int'>, {1: 72, 2: 58})\n",
      "4\n",
      "defaultdict(<class 'int'>, {1: 79, 2: 51})\n"
     ]
    }
   ],
   "source": [
    "counts = defaultdict(int)\n",
    "for ds in [t[1] for t in truth_dataset_format_tuples_cv]:\n",
    "    counts[ds] += 1\n",
    "print(counts)\n",
    "\n",
    "for k, (train_indices, test_indices) in enumerate(k_fold.split(wav_file_paths_cv)):\n",
    "    print(k)\n",
    "    counts_test_k = defaultdict(int)\n",
    "    for ds in [t[1] for t in [truth_dataset_format_tuples_cv[i] for i in test_indices]]:\n",
    "        counts_test_k[ds] += 1\n",
    "    print(counts_test_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, proba_threshold, list_of_X, y, epsilon=1e-7):\n",
    "    proba_matrix = model.predict(list_of_X)\n",
    "    y = proba_matrix > proba_threshold\n",
    "    y = y.astype(np.int8)\n",
    "\n",
    "    # Make sure at least one pitch is returned.\n",
    "    for probas, labels in zip(proba_matrix, y):\n",
    "        if labels.sum() == 0:\n",
    "            max_proba = max(probas)\n",
    "            max_index = np.where(np.logical_and(probas > max_proba - epsilon, probas < max_proba + epsilon))[0][0]\n",
    "            labels[max_index] = 1\n",
    "\n",
    "    return y\n",
    "\n",
    "def print_metrics(y, y_predicted):\n",
    "    print('Accuracy: {}'.format(accuracy_score(y, y_predicted)))\n",
    "    print(classification_report(y, y_predicted,\n",
    "                                target_names=[str(pitch) for pitch in range(min_pitch, max_pitch + 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_1(list_of_X, n_output_units):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(20, (7, 3), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Conv2D(20, (3, 3), padding='valid')(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(0.25)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(0.5)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_2(list_of_X, n_output_units, dropout_conv=0.25, dropout_dense=0.5):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(10, (7, 3), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Conv2D(20, (3, 3), padding='valid')(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(dropout_conv)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(dropout_dense)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    # model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_3(list_of_X, n_output_units):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(49, (16, 6), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 29))(conv)\n",
    "        conv = Dropout(0.25)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(32)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(0.5)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_4(list_of_X, n_output_units, dropout_conv=0.25, dropout_dense=0.5):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(10, (7, 3), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(dropout_conv)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(dropout_dense)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    # model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_5(list_of_X, n_output_units, dropout_conv=0.25, dropout_dense=0.5):\n",
    "    inputs = []\n",
    "    conv_blocks = []\n",
    "    for X in list_of_X:\n",
    "        spectrogram = Input(shape=X.shape[1:])\n",
    "        inputs.append(spectrogram)\n",
    "\n",
    "        conv = Conv2D(10, (7, 3), padding='valid')(spectrogram)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Conv2D(20, (3, 3), padding='valid')(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        conv = MaxPooling2D(pool_size=(1, 3))(conv)\n",
    "        conv = Dropout(dropout_conv)(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    z = Dense(256)(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Dropout(dropout_dense)(z)\n",
    "    output = Dense(n_output_units, activation='sigmoid')(z)\n",
    "\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2\n",
      "dropout_conv=0.2, dropout_dense=0.4, proba_threshold=0.5\n",
      "Accuracy: 0.8852672750977836\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       0.95      0.95      0.95        56\n",
      "         41       1.00      1.00      1.00        20\n",
      "         42       1.00      0.91      0.95        11\n",
      "         43       1.00      0.72      0.84        50\n",
      "         44       1.00      1.00      1.00        11\n",
      "         45       1.00      0.99      1.00       187\n",
      "         46       1.00      1.00      1.00        22\n",
      "         47       0.94      0.99      0.96       119\n",
      "         48       0.99      0.92      0.95       332\n",
      "         49       0.97      0.98      0.98       101\n",
      "         50       1.00      0.99      0.99       267\n",
      "         51       0.93      0.93      0.93        29\n",
      "         52       0.91      0.98      0.94       322\n",
      "         53       0.93      0.75      0.83       108\n",
      "         54       0.97      0.91      0.94       246\n",
      "         55       0.92      0.95      0.93       243\n",
      "         56       0.99      0.94      0.97       169\n",
      "         57       0.87      0.92      0.90       275\n",
      "         58       0.89      0.60      0.72        78\n",
      "         59       0.92      0.91      0.92       374\n",
      "         60       0.93      0.93      0.93       175\n",
      "         61       0.94      0.94      0.94       291\n",
      "         62       0.89      0.87      0.88       291\n",
      "         63       0.90      0.58      0.71        48\n",
      "         64       0.95      0.92      0.93       309\n",
      "         65       0.92      0.83      0.88        72\n",
      "         66       0.89      0.85      0.87        97\n",
      "         67       0.94      0.84      0.89       161\n",
      "         68       0.97      0.78      0.86        36\n",
      "         69       0.98      0.91      0.95       222\n",
      "         70       0.96      0.72      0.82        32\n",
      "         71       0.64      0.76      0.70        42\n",
      "         72       0.80      0.69      0.74        54\n",
      "         73       0.88      0.45      0.60        33\n",
      "         74       0.84      0.88      0.86        69\n",
      "         75       1.00      1.00      1.00        13\n",
      "         76       1.00      1.00      1.00        13\n",
      "         77       1.00      1.00      1.00         6\n",
      "         78       1.00      0.38      0.55        21\n",
      "         79       0.82      1.00      0.90         9\n",
      "         80       1.00      1.00      1.00         3\n",
      "         81       1.00      1.00      1.00         3\n",
      "         82       1.00      1.00      1.00         3\n",
      "         83       0.60      1.00      0.75         9\n",
      "         84       0.67      0.67      0.67         3\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.94      0.90      0.92      5038\n",
      "\n",
      "model_2\n",
      "dropout_conv=0.2, dropout_dense=0.4, proba_threshold=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\ProgramFiles\\Anaconda3_64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8889178617992177\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      0.96      0.98        56\n",
      "         41       1.00      1.00      1.00        20\n",
      "         42       1.00      1.00      1.00        11\n",
      "         43       1.00      0.68      0.81        50\n",
      "         44       1.00      1.00      1.00        11\n",
      "         45       1.00      0.97      0.99       187\n",
      "         46       1.00      0.95      0.98        22\n",
      "         47       0.94      0.98      0.96       119\n",
      "         48       0.98      0.93      0.96       332\n",
      "         49       0.99      1.00      1.00       101\n",
      "         50       1.00      0.99      0.99       267\n",
      "         51       1.00      1.00      1.00        29\n",
      "         52       0.92      0.98      0.95       322\n",
      "         53       0.95      0.73      0.83       108\n",
      "         54       0.97      0.90      0.93       246\n",
      "         55       0.89      0.95      0.92       243\n",
      "         56       0.99      0.96      0.98       169\n",
      "         57       0.92      0.92      0.92       275\n",
      "         58       0.86      0.62      0.72        78\n",
      "         59       0.92      0.91      0.92       374\n",
      "         60       0.94      0.93      0.93       175\n",
      "         61       0.93      0.96      0.94       291\n",
      "         62       0.87      0.87      0.87       291\n",
      "         63       0.92      0.48      0.63        48\n",
      "         64       0.94      0.92      0.93       309\n",
      "         65       0.95      0.85      0.90        72\n",
      "         66       0.93      0.88      0.90        97\n",
      "         67       0.95      0.88      0.92       161\n",
      "         68       1.00      0.75      0.86        36\n",
      "         69       0.97      0.93      0.95       222\n",
      "         70       0.92      0.69      0.79        32\n",
      "         71       0.64      0.76      0.70        42\n",
      "         72       0.76      0.65      0.70        54\n",
      "         73       0.94      0.48      0.64        33\n",
      "         74       0.87      0.90      0.89        69\n",
      "         75       0.93      1.00      0.96        13\n",
      "         76       1.00      1.00      1.00        13\n",
      "         77       1.00      1.00      1.00         6\n",
      "         78       0.94      0.76      0.84        21\n",
      "         79       1.00      1.00      1.00         9\n",
      "         80       1.00      1.00      1.00         3\n",
      "         81       1.00      0.67      0.80         3\n",
      "         82       1.00      1.00      1.00         3\n",
      "         83       1.00      1.00      1.00         9\n",
      "         84       0.50      0.67      0.57         3\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       0.50      0.67      0.57         3\n",
      "\n",
      "avg / total       0.94      0.91      0.92      5038\n",
      "\n",
      "model_2\n",
      "dropout_conv=0.2, dropout_dense=0.4, proba_threshold=0.5\n",
      "Accuracy: 0.891264667535854\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       0.96      0.96      0.96        56\n",
      "         41       0.95      1.00      0.98        20\n",
      "         42       0.92      1.00      0.96        11\n",
      "         43       0.97      0.72      0.83        50\n",
      "         44       1.00      1.00      1.00        11\n",
      "         45       1.00      0.98      0.99       187\n",
      "         46       1.00      0.95      0.98        22\n",
      "         47       0.93      0.97      0.95       119\n",
      "         48       0.99      0.94      0.96       332\n",
      "         49       0.98      0.99      0.99       101\n",
      "         50       1.00      0.98      0.99       267\n",
      "         51       0.97      0.97      0.97        29\n",
      "         52       0.90      0.98      0.94       322\n",
      "         53       0.95      0.77      0.85       108\n",
      "         54       0.96      0.93      0.95       246\n",
      "         55       0.95      0.95      0.95       243\n",
      "         56       0.99      0.96      0.97       169\n",
      "         57       0.91      0.93      0.92       275\n",
      "         58       0.88      0.65      0.75        78\n",
      "         59       0.92      0.94      0.93       374\n",
      "         60       0.96      0.93      0.94       175\n",
      "         61       0.93      0.94      0.94       291\n",
      "         62       0.89      0.87      0.88       291\n",
      "         63       0.91      0.60      0.72        48\n",
      "         64       0.96      0.93      0.95       309\n",
      "         65       0.95      0.85      0.90        72\n",
      "         66       0.91      0.89      0.90        97\n",
      "         67       0.93      0.84      0.88       161\n",
      "         68       1.00      0.72      0.84        36\n",
      "         69       0.97      0.93      0.95       222\n",
      "         70       0.92      0.75      0.83        32\n",
      "         71       0.61      0.79      0.69        42\n",
      "         72       0.78      0.65      0.71        54\n",
      "         73       0.88      0.45      0.60        33\n",
      "         74       0.86      0.90      0.88        69\n",
      "         75       1.00      1.00      1.00        13\n",
      "         76       0.59      1.00      0.74        13\n",
      "         77       1.00      1.00      1.00         6\n",
      "         78       0.90      0.43      0.58        21\n",
      "         79       1.00      1.00      1.00         9\n",
      "         80       1.00      1.00      1.00         3\n",
      "         81       1.00      1.00      1.00         3\n",
      "         82       1.00      1.00      1.00         3\n",
      "         83       1.00      1.00      1.00         9\n",
      "         84       1.00      1.00      1.00         3\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.94      0.91      0.92      5038\n",
      "\n",
      "model_2\n",
      "dropout_conv=0.2, dropout_dense=0.4, proba_threshold=0.5\n",
      "Accuracy: 0.8915254237288136\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       0.98      0.93      0.95        56\n",
      "         41       1.00      1.00      1.00        20\n",
      "         42       0.79      1.00      0.88        11\n",
      "         43       0.97      0.66      0.79        50\n",
      "         44       1.00      1.00      1.00        11\n",
      "         45       1.00      0.98      0.99       187\n",
      "         46       1.00      0.95      0.98        22\n",
      "         47       0.98      0.99      0.99       119\n",
      "         48       0.99      0.94      0.96       332\n",
      "         49       0.98      0.99      0.99       101\n",
      "         50       1.00      0.99      0.99       267\n",
      "         51       0.97      0.97      0.97        29\n",
      "         52       0.91      0.98      0.94       322\n",
      "         53       0.96      0.76      0.85       108\n",
      "         54       0.95      0.91      0.93       246\n",
      "         55       0.93      0.95      0.94       243\n",
      "         56       1.00      0.94      0.97       169\n",
      "         57       0.89      0.94      0.91       275\n",
      "         58       0.85      0.64      0.73        78\n",
      "         59       0.93      0.91      0.92       374\n",
      "         60       0.97      0.87      0.92       175\n",
      "         61       0.95      0.93      0.94       291\n",
      "         62       0.88      0.85      0.86       291\n",
      "         63       0.96      0.56      0.71        48\n",
      "         64       0.95      0.89      0.92       309\n",
      "         65       0.98      0.86      0.92        72\n",
      "         66       0.88      0.86      0.87        97\n",
      "         67       0.95      0.79      0.86       161\n",
      "         68       0.97      0.78      0.86        36\n",
      "         69       0.98      0.93      0.95       222\n",
      "         70       0.92      0.72      0.81        32\n",
      "         71       0.71      0.81      0.76        42\n",
      "         72       0.79      0.69      0.73        54\n",
      "         73       0.94      0.45      0.61        33\n",
      "         74       0.78      0.91      0.84        69\n",
      "         75       1.00      1.00      1.00        13\n",
      "         76       1.00      1.00      1.00        13\n",
      "         77       1.00      1.00      1.00         6\n",
      "         78       0.93      0.62      0.74        21\n",
      "         79       0.90      1.00      0.95         9\n",
      "         80       1.00      0.67      0.80         3\n",
      "         81       1.00      0.67      0.80         3\n",
      "         82       1.00      1.00      1.00         3\n",
      "         83       1.00      1.00      1.00         9\n",
      "         84       1.00      1.00      1.00         3\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       0.43      1.00      0.60         3\n",
      "\n",
      "avg / total       0.94      0.90      0.92      5038\n",
      "\n",
      "model_2\n",
      "dropout_conv=0.2, dropout_dense=0.4, proba_threshold=0.5\n",
      "Accuracy: 0.896219035202086\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         40       1.00      0.95      0.97        56\n",
      "         41       1.00      1.00      1.00        20\n",
      "         42       1.00      1.00      1.00        11\n",
      "         43       1.00      0.64      0.78        50\n",
      "         44       1.00      1.00      1.00        11\n",
      "         45       1.00      0.98      0.99       187\n",
      "         46       1.00      0.95      0.98        22\n",
      "         47       0.96      0.98      0.97       119\n",
      "         48       0.99      0.94      0.97       332\n",
      "         49       0.99      0.99      0.99       101\n",
      "         50       1.00      0.98      0.99       267\n",
      "         51       0.94      1.00      0.97        29\n",
      "         52       0.92      0.98      0.95       322\n",
      "         53       0.94      0.77      0.85       108\n",
      "         54       0.96      0.92      0.94       246\n",
      "         55       0.95      0.95      0.95       243\n",
      "         56       0.99      0.94      0.97       169\n",
      "         57       0.86      0.91      0.88       275\n",
      "         58       0.88      0.63      0.73        78\n",
      "         59       0.93      0.91      0.92       374\n",
      "         60       0.96      0.93      0.94       175\n",
      "         61       0.94      0.95      0.95       291\n",
      "         62       0.91      0.88      0.89       291\n",
      "         63       0.93      0.58      0.72        48\n",
      "         64       0.96      0.90      0.93       309\n",
      "         65       0.96      0.89      0.92        72\n",
      "         66       0.99      0.87      0.92        97\n",
      "         67       0.94      0.88      0.91       161\n",
      "         68       1.00      0.75      0.86        36\n",
      "         69       0.97      0.92      0.94       222\n",
      "         70       0.93      0.78      0.85        32\n",
      "         71       0.63      0.74      0.68        42\n",
      "         72       0.84      0.70      0.77        54\n",
      "         73       0.94      0.52      0.67        33\n",
      "         74       0.85      0.93      0.89        69\n",
      "         75       1.00      1.00      1.00        13\n",
      "         76       1.00      1.00      1.00        13\n",
      "         77       1.00      1.00      1.00         6\n",
      "         78       1.00      0.76      0.86        21\n",
      "         79       1.00      1.00      1.00         9\n",
      "         80       1.00      1.00      1.00         3\n",
      "         81       1.00      0.67      0.80         3\n",
      "         82       1.00      1.00      1.00         3\n",
      "         83       1.00      1.00      1.00         9\n",
      "         84       0.75      1.00      0.86         3\n",
      "         85       0.00      0.00      0.00         0\n",
      "         86       0.00      0.00      0.00         0\n",
      "         87       0.00      0.00      0.00         0\n",
      "         88       0.67      0.67      0.67         3\n",
      "\n",
      "avg / total       0.95      0.91      0.93      5038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(folds, create_model, dropout_conv, dropout_dense, proba_threshold):\n",
    "    y_test_all_folds = None\n",
    "    y_test_predicted_all_folds = None\n",
    "    for i, (list_of_X_train, y_train, list_of_X_test, y_test) in enumerate(folds):\n",
    "        # model_dir = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        # os.mkdir(model_dir)\n",
    "\n",
    "        model = create_model(list_of_X_train, max_pitch - min_pitch + 1,\n",
    "                             dropout_conv=dropout_conv, dropout_dense=dropout_dense)\n",
    "        model.fit(list_of_X_train, y_train,\n",
    "                  epochs=1000,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  sample_weight=None,\n",
    "                  class_weight=None,\n",
    "                  callbacks=[EarlyStopping(monitor='loss', patience=6),\n",
    "                             # ModelCheckpoint(os.path.join(model_dir, 'model.' + str(i) + '.{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                             #                 monitor='val_loss', save_best_only=True)\n",
    "                            ],\n",
    "                  verbose=0,\n",
    "                  # verbose=2,\n",
    "\n",
    "                  # validation_split=0.1,\n",
    "                  # validation_data=(list_of_X_test, y_test),\n",
    "                 )\n",
    "\n",
    "        # Load model with lowest val_loss\n",
    "        # path_to_model = max([os.path.join(model_dir, file)\n",
    "        #                      for file in os.listdir(model_dir)\n",
    "        #                      if file.startswith('model.' + str(i) + '.')])\n",
    "        # print(path_to_model)\n",
    "        # model = load_model(path_to_model)\n",
    "\n",
    "        y_test_predicted = predict(model, proba_threshold, list_of_X_test, y_test)\n",
    "        # print_metrics(y_test, y_test_predicted)\n",
    "\n",
    "        if y_test_all_folds is None:\n",
    "            y_test_all_folds = y_test\n",
    "        else:\n",
    "            y_test_all_folds = np.concatenate((y_test_all_folds, y_test))\n",
    "\n",
    "        if y_test_predicted_all_folds is None:\n",
    "            y_test_predicted_all_folds = y_test_predicted\n",
    "        else:\n",
    "            y_test_predicted_all_folds = np.concatenate((y_test_predicted_all_folds, y_test_predicted))\n",
    "\n",
    "    print_metrics(y_test_all_folds, y_test_predicted_all_folds)\n",
    "\n",
    "models = [\n",
    "    ('model_2', create_model_2),\n",
    "]\n",
    "for i in range(5):\n",
    "    for model_name, create_model in models:\n",
    "        print(model_name)\n",
    "        for dropout_conv in [0.2]:\n",
    "            for dropout_dense in [0.4]:\n",
    "                for proba_threshold in [0.5]:\n",
    "                    print('dropout_conv={}, dropout_dense={}, proba_threshold={}'.format(dropout_conv,\n",
    "                                                                                         dropout_dense,\n",
    "                                                                                         proba_threshold))\n",
    "                    train_and_evaluate(folds, create_model, dropout_conv, dropout_dense, proba_threshold)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
